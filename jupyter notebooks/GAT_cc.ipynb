{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing and Label Encoding\n",
    "def df_label_encoder(df, columns):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in columns:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df_label_encoder(df, ['merchant', 'category', 'city', 'state', 'job'])\n",
    "    df['amt'] = (df['amt'] - df['amt'].min()) / (df['amt'].max() - df['amt'].min())\n",
    "    df['node_from'] = df['cc_num'].astype(str)\n",
    "    df['node_to'] = df['merchant'].astype(str)\n",
    "    df = df.sort_values(by=['node_from'])\n",
    "    node_list = pd.concat([df['node_from'], df['node_to']]).unique()\n",
    "    return df, node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(df, node_list):\n",
    "    node_map = {node: idx for idx, node in enumerate(node_list)}\n",
    "    edge_index = np.array([\n",
    "        [node_map[from_node], node_map[to_node]] for from_node, to_node in zip(df['node_from'], df['node_to'])\n",
    "    ], dtype=np.int64).T\n",
    "    node_features = np.array(df[['amt', 'category', 'city', 'state']].values, dtype=np.float32)\n",
    "    labels = np.array(df['is_fraud'].values, dtype=np.int64)\n",
    "    return node_features, edge_index, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and preprocess\n",
    "df = pd.read_csv('creditcard/fraudTrain.csv')  # Update with your .csv file path\n",
    "\n",
    "# Reduce the dataset to 1/5th (20%) of the original size\n",
    "df = df.sample(frac=0.2, random_state=42)\n",
    "\n",
    "df, node_list = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph dataset\n",
    "node_features, edge_index, labels = create_graph_data(df, node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the WGAN Generator and Discriminator (same as before)\n",
    "class WGANGenerator(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(WGANGenerator, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, z):\n",
    "        x = self.fc1(z)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(WGANDiscriminator, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN parameters\n",
    "latent_dim = 16\n",
    "hidden_dim = 32\n",
    "input_dim = node_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = WGANGenerator(latent_dim, hidden_dim, input_dim)\n",
    "discriminator = WGANDiscriminator(input_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: WGAN Training for Class Balancing\n",
    "batch_size = 8\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = node_features[labels == 1]  # Fraudulent data (minority class)\n",
    "target_minority_class = np.sum(labels == 0)  # Non-fraudulent (majority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_data):\n",
    "    for _ in range(5):  # Update discriminator 5 times for every generator update\n",
    "        z = tf.random.normal([real_data.shape[0], latent_dim])\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake_data = generator(z, training=True)\n",
    "            d_real = discriminator(real_data, training=True)\n",
    "            d_fake = discriminator(fake_data, training=True)\n",
    "            d_loss = -tf.reduce_mean(d_real) + tf.reduce_mean(d_fake)\n",
    "\n",
    "        d_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        z = tf.random.normal([real_data.shape[0], latent_dim])\n",
    "        fake_data = generator(z, training=True)\n",
    "        g_loss = -tf.reduce_mean(discriminator(fake_data, training=True))\n",
    "\n",
    "    g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "\n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss D: 63.382938385009766, Loss G: 0.0034128082916140556\n",
      "Epoch 2/10, Loss D: 61.935787200927734, Loss G: 0.010986598208546638\n",
      "Epoch 3/10, Loss D: 60.50090789794922, Loss G: 0.01057228073477745\n",
      "Epoch 4/10, Loss D: 59.06543731689453, Loss G: 0.003476534504443407\n",
      "Epoch 5/10, Loss D: 57.61684799194336, Loss G: -0.0009680814109742641\n",
      "Epoch 6/10, Loss D: 56.168880462646484, Loss G: -0.005858915857970715\n",
      "Epoch 7/10, Loss D: 54.73793411254883, Loss G: -0.016547486186027527\n",
      "Epoch 8/10, Loss D: 53.296348571777344, Loss G: -0.005269114393740892\n",
      "Epoch 9/10, Loss D: 51.85076904296875, Loss G: -0.016116635873913765\n",
      "Epoch 10/10, Loss D: 50.40438461303711, Loss G: -0.020673412829637527\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    current_minority_count = np.sum(labels == 1)\n",
    "    if current_minority_count >= target_minority_class:\n",
    "        break\n",
    "\n",
    "    d_loss, g_loss = train_step(real_data)\n",
    "\n",
    "    # Update labels and node_features\n",
    "    z = tf.random.normal([real_data.shape[0], latent_dim])\n",
    "    generated_samples = generator(z)\n",
    "    labels = np.concatenate([labels, np.ones(generated_samples.shape[0], dtype=np.int64)])\n",
    "    node_features = np.concatenate([node_features, generated_samples.numpy()])\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss D: {d_loss.numpy()}, Loss G: {g_loss.numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: GAT Layer using TensorFlow (custom implementation)\n",
    "class GATLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, num_heads=1, dropout_rate=0.6):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.output_dim),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.attn_kernel = self.add_weight(shape=(2 * self.output_dim, 1),\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "    def call(self, node_features, adj_matrix, training=True):\n",
    "        h = tf.matmul(node_features, self.W)\n",
    "        num_nodes = h.shape[0]\n",
    "        h_expanded = tf.tile(tf.expand_dims(h, axis=0), [num_nodes, 1, 1])\n",
    "        h_expanded_transposed = tf.transpose(h_expanded, [1, 0, 2])\n",
    "\n",
    "        a_input = tf.concat([h_expanded, h_expanded_transposed], axis=-1)\n",
    "        e = self.leaky_relu(tf.matmul(a_input, self.attn_kernel))\n",
    "\n",
    "        attention = tf.where(adj_matrix > 0, e, tf.zeros_like(e))\n",
    "        attention = tf.nn.softmax(attention, axis=1)\n",
    "\n",
    "        h_prime = tf.matmul(attention, h)\n",
    "        if training:\n",
    "            h_prime = tf.nn.dropout(h_prime, rate=self.dropout_rate)\n",
    "\n",
    "        return tf.nn.elu(h_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(edge_index, num_nodes):\n",
    "    adj_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "    for i, j in zip(edge_index[0], edge_index[1]):\n",
    "        adj_matrix[i, j] = 1\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATModel(tf.keras.Model):\n",
    "    def __init__(self, num_features, hidden_size):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.gat_layer = GATLayer(output_dim=hidden_size)\n",
    "\n",
    "    def call(self, node_features, adj_matrix, training=False):\n",
    "        return self.gat_layer(node_features, adj_matrix, training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[274535,274535,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Tile]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-aabe9a41abed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0madj_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adjacency_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGATModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgat_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-d4a0e33e3a79>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, node_features, adj_matrix, training)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-b32829439595>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, node_features, adj_matrix, training)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnum_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mh_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mh_expanded_transposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  11517\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11518\u001b[0m       return tile_eager_fallback(\n\u001b[0;32m> 11519\u001b[0;31m           input, multiples, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m  11520\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11521\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile_eager_fallback\u001b[0;34m(input, multiples, name, ctx)\u001b[0m\n\u001b[1;32m  11557\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tmultiples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tmultiples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11558\u001b[0m   _result = _execute.execute(b\"Tile\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m> 11559\u001b[0;31m                              ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m  11560\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11561\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[274535,274535,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Tile]"
     ]
    }
   ],
   "source": [
    "# Step 5: Apply GAT on the reduced and balanced dataset\n",
    "adj_matrix = create_adjacency_matrix(edge_index, num_nodes=len(node_features))\n",
    "gat_model = GATModel(num_features=node_features.shape[1], hidden_size=128)\n",
    "gat_output = gat_model(node_features, adj_matrix, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output of GAT\n",
    "print(\"GAT output:\", gat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
