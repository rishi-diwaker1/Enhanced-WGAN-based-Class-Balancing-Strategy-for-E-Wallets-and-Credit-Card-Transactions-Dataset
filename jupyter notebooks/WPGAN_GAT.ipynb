{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girishkk/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/16], Loss D: -0.0685, Loss G: -0.0100\n",
      "Epoch [1/16], Loss D: -0.1109, Loss G: -0.0100\n",
      "Epoch [2/16], Loss D: -0.1531, Loss G: -0.0100\n",
      "Epoch [3/16], Loss D: -0.1972, Loss G: -0.0100\n",
      "Epoch [4/16], Loss D: -0.2439, Loss G: -0.0100\n",
      "Epoch [5/16], Loss D: -0.2925, Loss G: -0.0100\n",
      "Epoch [6/16], Loss D: -0.3428, Loss G: -0.0100\n",
      "Epoch [7/16], Loss D: -0.3959, Loss G: -0.0101\n",
      "Epoch [8/16], Loss D: -0.4509, Loss G: -0.0101\n",
      "Epoch [9/16], Loss D: -0.5068, Loss G: -0.0102\n",
      "Epoch [10/16], Loss D: -0.5647, Loss G: -0.0103\n",
      "Epoch [11/16], Loss D: -0.6245, Loss G: -0.0104\n",
      "Epoch [12/16], Loss D: -0.6860, Loss G: -0.0105\n",
      "Epoch [13/16], Loss D: -0.7493, Loss G: -0.0107\n",
      "Epoch [14/16], Loss D: -0.8141, Loss G: -0.0108\n",
      "Epoch [15/16], Loss D: -0.8803, Loss G: -0.0110\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GAT Layer implementation\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        # Linear transformation\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.a = nn.Linear(2 * out_features, 1, bias=False)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Linear transformation\n",
    "        h = self.W(x)\n",
    "        \n",
    "        # Prepare attention computation\n",
    "        N = h.size()[0]\n",
    "        \n",
    "        # Create edge feature matrix\n",
    "        edge_h = torch.cat((h[edge_index[0]], h[edge_index[1]]), dim=1)\n",
    "        \n",
    "        # Compute attention coefficients\n",
    "        edge_e = self.leakyrelu(self.a(edge_h))\n",
    "        \n",
    "        # Convert edge attention to sparse matrix format\n",
    "        edge_e = edge_e.squeeze()\n",
    "        attention = torch.zeros(N, N, device=x.device)\n",
    "        attention[edge_index[0], edge_index[1]] = edge_e\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = self.dropout_layer(attention)\n",
    "        \n",
    "        # Apply attention to node features\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        \n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "# Generator with GAT layers\n",
    "class GATGenerator(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_heads=4, dropout=0.6):\n",
    "        super(GATGenerator, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.fc_z = nn.Linear(latent_size, hidden_size)\n",
    "        \n",
    "        # Multiple GAT layers with different heads\n",
    "        self.gat1 = nn.ModuleList([\n",
    "            GATLayer(hidden_size, hidden_size // num_heads, dropout=dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        self.gat2 = nn.ModuleList([\n",
    "            GATLayer(hidden_size, hidden_size // num_heads, dropout=dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        # Transform latent vector\n",
    "        x = self.fc_z(z)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # First GAT layer with multi-head attention\n",
    "        x = torch.cat([att(x, edge_index) for att in self.gat1], dim=1)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Second GAT layer with multi-head attention\n",
    "        x = torch.cat([att(x, edge_index) for att in self.gat2], dim=1)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Generate final output\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Discriminator with GAT layers\n",
    "class GATDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads=4, dropout=0.6):\n",
    "        super(GATDiscriminator, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Multiple GAT layers with different heads\n",
    "        self.gat1 = nn.ModuleList([\n",
    "            GATLayer(input_size, hidden_size // num_heads, dropout=dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        self.gat2 = nn.ModuleList([\n",
    "            GATLayer(hidden_size, hidden_size // num_heads, dropout=dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GAT layer with multi-head attention\n",
    "        x = torch.cat([att(x, edge_index) for att in self.gat1], dim=1)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Second GAT layer with multi-head attention\n",
    "        x = torch.cat([att(x, edge_index) for att in self.gat2], dim=1)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Data preprocessing functions\n",
    "def df_label_encoder(df, columns):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in columns:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df_label_encoder(df, ['merchant', 'category', 'city', 'state', 'job'])\n",
    "    df['amt'] = (df['amt'] - df['amt'].min()) / (df['amt'].max() - df['amt'].min())\n",
    "    df['node_from'] = df['cc_num'].astype(str)\n",
    "    df['node_to'] = df['merchant'].astype(str)\n",
    "    df = df.sort_values(by=['node_from'])\n",
    "    node_list = pd.concat([df['node_from'], df['node_to']]).unique()\n",
    "    return df, node_list\n",
    "\n",
    "def create_graph_data(df, node_list):\n",
    "    node_map = {node: idx for idx, node in enumerate(node_list)}\n",
    "    edge_index = np.array([\n",
    "        [node_map[from_node], node_map[to_node]] \n",
    "        for from_node, to_node in zip(df['node_from'], df['node_to'])\n",
    "    ], dtype=np.int64).T\n",
    "    \n",
    "    node_features = torch.tensor(df[['amt', 'category', 'city', 'state']].values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    labels = torch.tensor(df['is_fraud'].values, dtype=torch.long)\n",
    "    \n",
    "    return node_features, edge_index, labels\n",
    "\n",
    "# Training function for GAT-WGAN\n",
    "def train_gat_wgan(generator, discriminator, node_features, edge_index, labels, \n",
    "                   num_epochs=16, batch_size=32, critic_iterations=5):\n",
    "    \n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    \n",
    "    real_data = node_features[labels == 1]\n",
    "    target_minority_class = torch.sum(labels == 0)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train Discriminator\n",
    "        for _ in range(critic_iterations):\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            # Real data\n",
    "            d_real = discriminator(real_data, edge_index)\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(real_data.size(0), latent_size)\n",
    "            fake_data = generator(z, edge_index)\n",
    "            d_fake = discriminator(fake_data.detach(), edge_index)\n",
    "            \n",
    "            # Compute WGAN loss\n",
    "            loss_d = -torch.mean(d_real) + torch.mean(d_fake)\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Clip weights\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_data = generator(torch.randn(real_data.size(0), latent_size), edge_index)\n",
    "        loss_g = -torch.mean(discriminator(fake_data, edge_index))\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}')\n",
    "    \n",
    "    return generator, discriminator\n",
    "\n",
    "# Classifier for evaluation\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads=4):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.gat = GATLayer(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x, edge_index=None):\n",
    "        if edge_index is not None:\n",
    "            x = self.gat(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('creditcard/fraudTrain.csv')\n",
    "    df, node_list = preprocess(df)\n",
    "    node_features, edge_index, labels = create_graph_data(df, node_list)\n",
    "    \n",
    "    # Model parameters\n",
    "    input_size = node_features.shape[1]\n",
    "    hidden_size = 128\n",
    "    latent_size = 64\n",
    "    num_heads = 4\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = GATGenerator(latent_size, hidden_size, input_size, num_heads=num_heads)\n",
    "    discriminator = GATDiscriminator(input_size, hidden_size, num_heads=num_heads)\n",
    "    \n",
    "    # Train models\n",
    "    generator, discriminator = train_gat_wgan(\n",
    "        generator, discriminator, node_features, edge_index, labels\n",
    "    )\n",
    "    \n",
    "    # Generate samples\n",
    "    num_samples = torch.sum(labels == 0) - torch.sum(labels == 1)\n",
    "    z = torch.randn(num_samples, latent_size)\n",
    "    generated_samples = generator(z, edge_index)\n",
    "    \n",
    "    # Combine real and generated data\n",
    "    augmented_features = torch.cat([node_features, generated_samples], dim=0)\n",
    "    augmented_labels = torch.cat([\n",
    "        labels, \n",
    "        torch.ones(num_samples, dtype=torch.long)\n",
    "    ])\n",
    "    \n",
    "    # Split data for classifier training\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        augmented_features, augmented_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate classifier\n",
    "    classifier = Classifier(input_size, hidden_size, num_heads)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop for classifier\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = classifier(x_train, edge_index)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            classifier.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = classifier(x_test, edge_index)\n",
    "                test_loss = criterion(test_outputs, y_test)\n",
    "                accuracy = accuracy_score(\n",
    "                    y_test.cpu().numpy(),\n",
    "                    test_outputs.argmax(dim=1).cpu().numpy()\n",
    "                )\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item():.4f}, Test Acc: {accuracy:.4f}')\n",
    "    \n",
    "    # Final evaluation\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        final_outputs = classifier(x_test, edge_index)\n",
    "        y_pred = final_outputs.argmax(dim=1)\n",
    "        final_accuracy = accuracy_score(y_test.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "        \n",
    "        # Calculate ROC AUC\n",
    "        y_proba = F.softmax(final_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "        roc_auc = roc_auc_score(y_test.cpu().numpy(), y_proba)\n",
    "        print(f'ROC AUC Score: {roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
