{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing and Label Encoding\n",
    "def df_label_encoder(df, columns):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in columns:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df_label_encoder(df, ['merchant', 'category', 'city', 'state', 'job'])\n",
    "    df['amt'] = (df['amt'] - df['amt'].min()) / (df['amt'].max() - df['amt'].min())\n",
    "    df['node_from'] = df['cc_num'].astype(str)\n",
    "    df['node_to'] = df['merchant'].astype(str)\n",
    "    df = df.sort_values(by=['node_from'])\n",
    "    node_list = pd.concat([df['node_from'], df['node_to']]).unique()\n",
    "    return df, node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(df, node_list):\n",
    "    node_map = {node: idx for idx, node in enumerate(node_list)}\n",
    "    edge_index = np.array([\n",
    "        [node_map[from_node], node_map[to_node]] for from_node, to_node in zip(df['node_from'], df['node_to'])\n",
    "    ], dtype=np.int64).T\n",
    "    node_features = np.array(df[['amt', 'category', 'city', 'state']].values, dtype=np.float32)\n",
    "    labels = np.array(df['is_fraud'].values, dtype=np.int64)\n",
    "    return node_features, edge_index, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and preprocess\n",
    "df = pd.read_csv('creditcard/fraudTrain.csv')  # Update with your .csv file path\n",
    "df = df.sample(frac=0.2, random_state=42)\n",
    "df, node_list = preprocess(df)\n",
    "node_features, edge_index, labels = create_graph_data(df, node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adjacency matrix for GAT layer\n",
    "def create_adjacency_matrix(edge_index, num_nodes):\n",
    "    adj_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "    for i, j in zip(edge_index[0], edge_index[1]):\n",
    "        adj_matrix[i, j] = 1\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = create_adjacency_matrix(edge_index, num_nodes=len(node_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define GAT Layer\n",
    "class GATLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, num_heads=1, dropout_rate=0.6):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.output_dim),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.attn_kernel = self.add_weight(shape=(2 * self.output_dim, 1),\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "    def call(self, node_features, adj_matrix, training=True):\n",
    "        h = tf.matmul(node_features, self.W)\n",
    "        num_nodes = h.shape[0]\n",
    "        h_expanded = tf.tile(tf.expand_dims(h, axis=0), [num_nodes, 1, 1])\n",
    "        h_expanded_transposed = tf.transpose(h_expanded, [1, 0, 2])\n",
    "\n",
    "        a_input = tf.concat([h_expanded, h_expanded_transposed], axis=-1)\n",
    "        e = self.leaky_relu(tf.matmul(a_input, self.attn_kernel))\n",
    "\n",
    "        attention = tf.where(adj_matrix > 0, e, tf.zeros_like(e))\n",
    "        attention = tf.nn.softmax(attention, axis=1)\n",
    "\n",
    "        h_prime = tf.matmul(attention, h)\n",
    "        if training:\n",
    "            h_prime = tf.nn.dropout(h_prime, rate=self.dropout_rate)\n",
    "\n",
    "        return tf.nn.elu(h_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define WGAN with GAT layers in Generator and Discriminator\n",
    "class WGANGenerator(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(WGANGenerator, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.gat_layer = GATLayer(output_dim=output_dim)\n",
    "        self.fc2 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, z, adj_matrix, training=True):\n",
    "        x = self.fc1(z)\n",
    "        x = self.gat_layer(x, adj_matrix, training=training)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(WGANDiscriminator, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.gat_layer = GATLayer(output_dim=hidden_dim)\n",
    "        self.fc2 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x, adj_matrix, training=True):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gat_layer(x, adj_matrix, training=training)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN parameters\n",
    "latent_dim = 8\n",
    "hidden_dim = 16\n",
    "input_dim = node_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = WGANGenerator(latent_dim, hidden_dim, input_dim)\n",
    "discriminator = WGANDiscriminator(input_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: WGAN Training for Class Balancing\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "real_data = node_features[labels == 1]\n",
    "target_minority_class = np.sum(labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_data, adj_matrix):\n",
    "    for _ in range(5):  # Update discriminator 5 times for every generator update\n",
    "        z = tf.random.normal([real_data.shape[0], latent_dim])\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake_data = generator(z, adj_matrix, training=True)\n",
    "            d_real = discriminator(real_data, adj_matrix, training=True)\n",
    "            d_fake = discriminator(fake_data, adj_matrix, training=True)\n",
    "            d_loss = -tf.reduce_mean(d_real) + tf.reduce_mean(d_fake)\n",
    "\n",
    "        d_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        z = tf.random.normal([real_data.shape[0], latent_dim])\n",
    "        fake_data = generator(z, adj_matrix, training=True)\n",
    "        g_loss = -tf.reduce_mean(discriminator(fake_data, adj_matrix, training=True))\n",
    "\n",
    "    g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "\n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    current_minority_count = np.sum(labels == 1)\n",
    "    if current_minority_count >= target_minority_class:\n",
    "        break\n",
    "\n",
    "    d_loss, g_loss = train_step(real_data, adj_matrix)\n",
    "\n",
    "    # Update labels and node_features\n",
    "    z = tf.random.normal([real_data.shape[0], latent_dim])\n",
    "    generated_samples = generator(z, adj_matrix)\n",
    "    labels = np.concatenate([labels, np.ones(generated_samples.shape[0], dtype=np.int64)])\n",
    "    node_features = np.concatenate([node_features, generated_samples.numpy()])\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss D: {d_loss.numpy()}, Loss G: {g_loss.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Final distribution of classes in a bar graph\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "plt.bar(unique, counts, tick_label=['Non-Fraud', 'Fraud'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Final Distribution of Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
