{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girishkk/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/16], Loss D: -0.0333, Loss G: 0.0246\n",
      "Epoch [1/16], Loss D: -0.0677, Loss G: 0.0412\n",
      "Epoch [2/16], Loss D: -0.0962, Loss G: 0.0517\n",
      "Epoch [3/16], Loss D: -0.1148, Loss G: 0.0580\n",
      "Epoch [4/16], Loss D: -0.1283, Loss G: 0.0625\n",
      "Epoch [5/16], Loss D: -0.1381, Loss G: 0.0658\n",
      "Epoch [6/16], Loss D: -0.1460, Loss G: 0.0680\n",
      "Epoch [7/16], Loss D: -0.1504, Loss G: 0.0693\n",
      "Epoch [8/16], Loss D: -0.1527, Loss G: 0.0702\n",
      "Epoch [9/16], Loss D: -0.1541, Loss G: 0.0708\n",
      "Epoch [10/16], Loss D: -0.1551, Loss G: 0.0711\n",
      "Epoch [11/16], Loss D: -0.1560, Loss G: 0.0715\n",
      "Epoch [12/16], Loss D: -0.1567, Loss G: 0.0717\n",
      "Epoch [13/16], Loss D: -0.1573, Loss G: 0.0719\n",
      "Epoch [14/16], Loss D: -0.1579, Loss G: 0.0721\n",
      "Epoch [15/16], Loss D: -0.1583, Loss G: 0.0722\n",
      "Epoch 0, Loss: 3.3216, Test Acc: 0.9971\n",
      "Epoch 5, Loss: 0.5495, Test Acc: 0.9971\n",
      "Epoch 10, Loss: 0.5920, Test Acc: 0.9971\n",
      "Epoch 15, Loss: 0.5837, Test Acc: 0.9971\n",
      "Epoch 20, Loss: 0.5433, Test Acc: 0.9971\n",
      "Epoch 25, Loss: 0.4941, Test Acc: 0.9971\n",
      "Final Test Accuracy: 0.9971\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GraphSAGE Layer implementation\n",
    "class GraphSAGELayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True):\n",
    "        super(GraphSAGELayer, self).__init__()\n",
    "        self.normalize = normalize\n",
    "        self.weight = nn.Linear(in_channels * 2, out_channels, bias=True)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.weight)\n",
    "        if self.weight.bias is not None:\n",
    "            nn.init.zeros_(self.weight.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Aggregate neighbor features\n",
    "        row, col = edge_index\n",
    "        neighbor_features = x[col]\n",
    "        neighbor_mean = scatter_mean(neighbor_features, row, dim=0, dim_size=x.size(0))\n",
    "        \n",
    "        # Concatenate self features with aggregated neighbor features\n",
    "        out = torch.cat([x, neighbor_mean], dim=1)\n",
    "        out = self.weight(out)\n",
    "        \n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2, dim=1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Modified Generator with GraphSAGE layers\n",
    "class GraphSAGEGenerator(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_layers=2):\n",
    "        super(GraphSAGEGenerator, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.fc_z = nn.Linear(latent_size, hidden_size)\n",
    "        \n",
    "        # GraphSAGE layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = hidden_size if i == 0 else hidden_size\n",
    "            self.conv_layers.append(GraphSAGELayer(in_channels, hidden_size))\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        # Transform latent vector\n",
    "        x = self.fc_z(z)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Apply GraphSAGE layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.conv_layers[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # Generate final output\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Modified Discriminator with GraphSAGE layers\n",
    "class GraphSAGEDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super(GraphSAGEDiscriminator, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GraphSAGE layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        first_layer = GraphSAGELayer(input_size, hidden_size)\n",
    "        self.conv_layers.append(first_layer)\n",
    "        \n",
    "        for i in range(num_layers - 1):\n",
    "            self.conv_layers.append(GraphSAGELayer(hidden_size, hidden_size))\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply GraphSAGE layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.conv_layers[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        # Final classification\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n",
    "# Add the missing Classifier class\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 2)  # 2 classes: fraud and non-fraud\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Helper function for GraphSAGE\n",
    "def scatter_mean(src, index, dim=-1, dim_size=None):\n",
    "    if dim_size is None:\n",
    "        dim_size = index.max().item() + 1\n",
    "    \n",
    "    out = torch.zeros((dim_size, src.size(-1)), device=src.device)\n",
    "    count = torch.zeros(dim_size, device=src.device)\n",
    "    \n",
    "    index_expanded = index.unsqueeze(-1).expand(-1, src.size(-1))\n",
    "    out.scatter_add_(0, index_expanded, src)\n",
    "    count.scatter_add_(0, index, torch.ones_like(index, dtype=torch.float))\n",
    "    \n",
    "    count[count == 0] = 1  # Avoid division by zero\n",
    "    count = count.unsqueeze(-1)\n",
    "    \n",
    "    return out / count\n",
    "\n",
    "# Data preprocessing functions (same as before)\n",
    "def df_label_encoder(df, columns):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in columns:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df_label_encoder(df, ['merchant', 'category', 'city', 'state', 'job'])\n",
    "    df['amt'] = (df['amt'] - df['amt'].min()) / (df['amt'].max() - df['amt'].min())\n",
    "    df['node_from'] = df['cc_num'].astype(str)\n",
    "    df['node_to'] = df['merchant'].astype(str)\n",
    "    df = df.sort_values(by=['node_from'])\n",
    "    node_list = pd.concat([df['node_from'], df['node_to']]).unique()\n",
    "    return df, node_list\n",
    "\n",
    "def create_graph_data(df, node_list):\n",
    "    node_map = {node: idx for idx, node in enumerate(node_list)}\n",
    "    edge_index = np.array([\n",
    "        [node_map[from_node], node_map[to_node]] \n",
    "        for from_node, to_node in zip(df['node_from'], df['node_to'])\n",
    "    ], dtype=np.int64).T\n",
    "    \n",
    "    node_features = torch.tensor(df[['amt', 'category', 'city', 'state']].values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    labels = torch.tensor(df['is_fraud'].values, dtype=torch.long)\n",
    "    \n",
    "    return node_features, edge_index, labels\n",
    "\n",
    "# Training parameters\n",
    "def train_wgan_graphsage(generator, discriminator, node_features, edge_index, labels, \n",
    "                        num_epochs=16, batch_size=32, critic_iterations=5):\n",
    "    \n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    \n",
    "    real_data = node_features[labels == 1]\n",
    "    target_minority_class = torch.sum(labels == 0)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train Discriminator\n",
    "        for _ in range(critic_iterations):\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            # Real data\n",
    "            d_real = discriminator(real_data, edge_index)\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(real_data.size(0), latent_size)\n",
    "            fake_data = generator(z, edge_index)\n",
    "            d_fake = discriminator(fake_data.detach(), edge_index)\n",
    "            \n",
    "            # Compute WGAN loss\n",
    "            loss_d = -torch.mean(d_real) + torch.mean(d_fake)\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Clip weights\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_data = generator(torch.randn(real_data.size(0), latent_size), edge_index)\n",
    "        loss_g = -torch.mean(discriminator(fake_data, edge_index))\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}')\n",
    "    \n",
    "    return generator, discriminator\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('creditcard/fraudTrain.csv')\n",
    "    df, node_list = preprocess(df)\n",
    "    node_features, edge_index, labels = create_graph_data(df, node_list)\n",
    "    \n",
    "    # Model parameters\n",
    "    input_size = node_features.shape[1]\n",
    "    hidden_size = 128\n",
    "    latent_size = 64\n",
    "    num_layers = 2\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = GraphSAGEGenerator(latent_size, hidden_size, input_size, num_layers)\n",
    "    discriminator = GraphSAGEDiscriminator(input_size, hidden_size, num_layers)\n",
    "    \n",
    "    # Train models\n",
    "    generator, discriminator = train_wgan_graphsage(\n",
    "        generator, discriminator, node_features, edge_index, labels\n",
    "    )\n",
    "    \n",
    "    # Generate samples\n",
    "    num_samples = torch.sum(labels == 0) - torch.sum(labels == 1)\n",
    "    z = torch.randn(num_samples, latent_size)\n",
    "    with torch.no_grad():  # Add no_grad here for generation\n",
    "        generated_samples = generator(z, edge_index)\n",
    "    \n",
    "    # Combine real and generated data\n",
    "    augmented_features = torch.cat([node_features, generated_samples], dim=0)\n",
    "    augmented_labels = torch.cat([\n",
    "        labels, \n",
    "        torch.ones(num_samples, dtype=torch.long)\n",
    "    ])\n",
    "    \n",
    "    # Split data for classifier training\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        augmented_features.detach(),  # Add detach here\n",
    "        augmented_labels, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate classifier\n",
    "    classifier = Classifier(input_size, hidden_size)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop for classifier\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = classifier(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Move zero_grad before loss calculation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        if epoch % 5 == 0:\n",
    "            classifier.eval()\n",
    "            with torch.no_grad():  # Add no_grad for evaluation\n",
    "                test_outputs = classifier(x_test)\n",
    "                test_loss = criterion(test_outputs, y_test)\n",
    "                accuracy = accuracy_score(\n",
    "                    y_test.cpu().numpy(),\n",
    "                    test_outputs.argmax(dim=1).cpu().numpy()\n",
    "                )\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item():.4f}, Test Acc: {accuracy:.4f}')\n",
    "    \n",
    "    # Final evaluation\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        final_outputs = classifier(x_test)\n",
    "        y_pred = final_outputs.argmax(dim=1)\n",
    "        final_accuracy = accuracy_score(y_test.cpu().numpy(), y_pred.cpu().numpy())\n",
    "        print(f'Final Test Accuracy: {final_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and preprocessing data...\n",
      "Initializing models...\n",
      "Training WGAN-GP...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 634.00 MiB (GPU 0; 31.74 GiB total capacity; 58.04 MiB already allocated; 404.94 MiB free; 82.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e55f61b16ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training WGAN-GP...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     generator, discriminator = train_wgan_graphsage(\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e55f61b16ae6>\u001b[0m in \u001b[0;36mtrain_wgan_graphsage\u001b[0;34m(generator, discriminator, node_features, edge_index, labels, num_epochs, batch_size, critic_iterations)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0moptimizer_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0md_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Generate noise on the same device as other tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e55f61b16ae6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e55f61b16ae6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mneighbor_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mneighbor_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbor_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 634.00 MiB (GPU 0; 31.74 GiB total capacity; 58.04 MiB already allocated; 404.94 MiB free; 82.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Metrics tracking class\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "        self.test_epochs = []  # To track epochs where test metrics are recorded\n",
    "    \n",
    "    def update(self, epoch, train_loss, test_loss=None, test_accuracy=None):\n",
    "        self.train_losses.append(train_loss)\n",
    "        if test_loss is not None:\n",
    "            self.test_losses.append(test_loss)\n",
    "            self.test_accuracies.append(test_accuracy)\n",
    "            self.test_epochs.append(epoch)\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Training and Test Loss plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        train_epochs = range(1, len(self.train_losses) + 1)\n",
    "        plt.plot(train_epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        \n",
    "        if self.test_losses:\n",
    "            plt.plot(self.test_epochs, self.test_losses, 'r-', marker='o', label='Test Loss')\n",
    "        \n",
    "        plt.title('Training and Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Test Accuracy plot\n",
    "        if self.test_accuracies:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(self.test_epochs, self.test_accuracies, 'g-', marker='o', label='Test Accuracy')\n",
    "            plt.title('Test Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Helper function for GraphSAGE\n",
    "def scatter_mean(src, index, dim=-1, dim_size=None):\n",
    "    if dim_size is None:\n",
    "        dim_size = index.max().item() + 1\n",
    "    \n",
    "    out = torch.zeros((dim_size, src.size(-1)), device=src.device)\n",
    "    count = torch.zeros(dim_size, device=src.device)\n",
    "    \n",
    "    index_expanded = index.unsqueeze(-1).expand(-1, src.size(-1))\n",
    "    out.scatter_add_(0, index_expanded, src)\n",
    "    count.scatter_add_(0, index, torch.ones_like(index, dtype=torch.float))\n",
    "    \n",
    "    count[count == 0] = 1\n",
    "    count = count.unsqueeze(-1)\n",
    "    \n",
    "    return out / count\n",
    "\n",
    "# GraphSAGE Layer implementation\n",
    "class GraphSAGELayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True):\n",
    "        super(GraphSAGELayer, self).__init__()\n",
    "        self.normalize = normalize\n",
    "        self.weight = nn.Linear(in_channels * 2, out_channels, bias=True)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.weight)\n",
    "        if self.weight.bias is not None:\n",
    "            nn.init.zeros_(self.weight.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        neighbor_features = x[col]\n",
    "        neighbor_mean = scatter_mean(neighbor_features, row, dim=0, dim_size=x.size(0))\n",
    "        \n",
    "        out = torch.cat([x, neighbor_mean], dim=1)\n",
    "        out = self.weight(out)\n",
    "        \n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2, dim=1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Generator with GraphSAGE layers\n",
    "class GraphSAGEGenerator(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_layers=2):\n",
    "        super(GraphSAGEGenerator, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.fc_z = nn.Linear(latent_size, hidden_size)\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = hidden_size if i == 0 else hidden_size\n",
    "            self.conv_layers.append(GraphSAGELayer(in_channels, hidden_size))\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        x = self.fc_z(z)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.conv_layers[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Discriminator with GraphSAGE layers\n",
    "class GraphSAGEDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super(GraphSAGEDiscriminator, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        first_layer = GraphSAGELayer(input_size, hidden_size)\n",
    "        self.conv_layers.append(first_layer)\n",
    "        \n",
    "        for i in range(num_layers - 1):\n",
    "            self.conv_layers.append(GraphSAGELayer(hidden_size, hidden_size))\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.conv_layers[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Classifier for final prediction\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 2)  # binary classification\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Data preprocessing functions\n",
    "def df_label_encoder(df, columns):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in columns:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df_label_encoder(df, ['merchant', 'category', 'city', 'state', 'job'])\n",
    "    df['amt'] = (df['amt'] - df['amt'].min()) / (df['amt'].max() - df['amt'].min())\n",
    "    df['node_from'] = df['cc_num'].astype(str)\n",
    "    df['node_to'] = df['merchant'].astype(str)\n",
    "    df = df.sort_values(by=['node_from'])\n",
    "    node_list = pd.concat([df['node_from'], df['node_to']]).unique()\n",
    "    return df, node_list\n",
    "\n",
    "def create_graph_data(df, node_list):\n",
    "    node_map = {node: idx for idx, node in enumerate(node_list)}\n",
    "    edge_index = np.array([[node_map[from_node], node_map[to_node]] \n",
    "                          for from_node, to_node in zip(df['node_from'], df['node_to'])], \n",
    "                         dtype=np.int64).T\n",
    "    \n",
    "    node_features = torch.tensor(df[['amt', 'category', 'city', 'state']].values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    labels = torch.tensor(df['is_fraud'].values, dtype=torch.long)\n",
    "    \n",
    "    return node_features, edge_index, labels\n",
    "\n",
    "# WGAN training function\n",
    "def train_wgan_graphsage(generator, discriminator, node_features, edge_index, labels, \n",
    "                        num_epochs=16, batch_size=32, critic_iterations=5):\n",
    "    \n",
    "    device = node_features.device  # Get the device from input tensors\n",
    "    \n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    \n",
    "    real_data = node_features[labels == 1]\n",
    "    latent_size = generator.fc_z.in_features  # Get latent size from generator's first layer\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train Discriminator\n",
    "        for _ in range(critic_iterations):\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            d_real = discriminator(real_data, edge_index)\n",
    "            \n",
    "            # Generate noise on the same device as other tensors\n",
    "            z = torch.randn(real_data.size(0), latent_size, device=device)\n",
    "            fake_data = generator(z, edge_index)\n",
    "            d_fake = discriminator(fake_data.detach(), edge_index)\n",
    "            \n",
    "            loss_d = -torch.mean(d_real) + torch.mean(d_fake)\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Clip discriminator weights\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        z = torch.randn(real_data.size(0), latent_size, device=device)  # Generate noise on correct device\n",
    "        fake_data = generator(z, edge_index)\n",
    "        loss_g = -torch.mean(discriminator(fake_data, edge_index))\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}')\n",
    "    \n",
    "    return generator, discriminator\n",
    "\n",
    "def generate_synthetic_samples(generator, num_samples, edge_index, device):\n",
    "    latent_size = generator.fc_z.in_features\n",
    "    z = torch.randn(num_samples, latent_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        generated_samples = generator(z, edge_index)\n",
    "    return generated_samples\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = pd.read_csv('creditcard/fraudTrain.csv')\n",
    "    df, node_list = preprocess(df)\n",
    "    node_features, edge_index, labels = create_graph_data(df, node_list)\n",
    "    \n",
    "    # Move data to device\n",
    "    node_features = node_features.to(device)\n",
    "    edge_index = edge_index.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Model parameters\n",
    "    input_size = node_features.shape[1]\n",
    "    hidden_size = 128\n",
    "    latent_size = 64\n",
    "    num_layers = 2\n",
    "    \n",
    "    # Initialize models\n",
    "    print(\"Initializing models...\")\n",
    "    generator = GraphSAGEGenerator(latent_size, hidden_size, input_size, num_layers).to(device)\n",
    "    discriminator = GraphSAGEDiscriminator(input_size, hidden_size, num_layers).to(device)\n",
    "    \n",
    "    # Train WGAN-GP\n",
    "    print(\"Training WGAN-GP...\")\n",
    "    generator, discriminator = train_wgan_graphsage(\n",
    "        generator, discriminator, node_features, edge_index, labels\n",
    "    )\n",
    "    \n",
    "    # Generate synthetic samples\n",
    "    print(\"Generating synthetic samples...\")\n",
    "    num_samples = torch.sum(labels == 0) - torch.sum(labels == 1)\n",
    "    generated_samples = generate_synthetic_samples(generator, num_samples, edge_index, device)\n",
    "    \n",
    "    # Combine real and generated data\n",
    "    augmented_features = torch.cat([node_features, generated_samples], dim=0)\n",
    "    augmented_labels = torch.cat([\n",
    "        labels, \n",
    "        torch.ones(num_samples, dtype=torch.long, device=device)\n",
    "    ])\n",
    "    \n",
    "    # Split data for classifier training\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        augmented_features.cpu().numpy(),\n",
    "        augmented_labels.cpu().numpy(), \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert split data to tensors and move to device\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float).to(device)\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Initialize classifier and training components\n",
    "    print(\"Training classifier...\")\n",
    "    classifier = Classifier(input_size, hidden_size).to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metrics_tracker = MetricsTracker()\n",
    "    \n",
    "    # Training loop for classifier\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = classifier(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        if epoch % 5 == 0:\n",
    "            classifier.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = classifier(x_test)\n",
    "                test_loss = criterion(test_outputs, y_test)\n",
    "                accuracy = accuracy_score(\n",
    "                    y_test.cpu().numpy(),\n",
    "                    test_outputs.argmax(dim=1).cpu().numpy()\n",
    "                )\n",
    "                metrics_tracker.update(\n",
    "                    epoch=epoch,\n",
    "                    train_loss=loss.item(),\n",
    "                    test_loss=test_loss.item(),\n",
    "                    test_accuracy=accuracy\n",
    "                )\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item():.4f}, Test Acc: {accuracy:.4f}')\n",
    "        else:\n",
    "            metrics_tracker.update(epoch=epoch, train_loss=loss.item())\n",
    "    \n",
    "    # Plot training metrics\n",
    "    metrics_tracker.plot_metrics()\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        final_outputs = classifier(x_test)\n",
    "        y_pred = final_outputs.argmax(dim=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        final_accuracy = accuracy_score(y_test.cpu().numpy(), y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, accuracy_score, confusion_matrix,\n",
    "    precision_score, recall_score, matthews_corrcoef, r2_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Enhanced Metrics tracking class\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "        self.test_precisions = []\n",
    "        self.test_recalls = []\n",
    "        self.test_f1s = []\n",
    "        self.test_mccs = []\n",
    "        self.test_r2s = []\n",
    "        self.test_roc_aucs = []\n",
    "        self.test_epochs = []\n",
    "    \n",
    "    def update(self, epoch, train_loss, test_loss=None, metrics=None):\n",
    "        self.train_losses.append(train_loss)\n",
    "        if test_loss is not None and metrics is not None:\n",
    "            self.test_losses.append(test_loss)\n",
    "            self.test_accuracies.append(metrics['accuracy'])\n",
    "            self.test_precisions.append(metrics['precision'])\n",
    "            self.test_recalls.append(metrics['recall'])\n",
    "            self.test_f1s.append(metrics['f1'])\n",
    "            self.test_mccs.append(metrics['mcc'])\n",
    "            self.test_r2s.append(metrics['r2'])\n",
    "            self.test_roc_aucs.append(metrics['roc_auc'])\n",
    "            self.test_epochs.append(epoch)\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Training and Test Loss plot\n",
    "        plt.subplot(2, 3, 1)\n",
    "        train_epochs = range(1, len(self.train_losses) + 1)\n",
    "        plt.plot(train_epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        if self.test_losses:\n",
    "            plt.plot(self.test_epochs, self.test_losses, 'r-', marker='o', label='Test Loss')\n",
    "        plt.title('Training and Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Accuracy, Precision, Recall plot\n",
    "        plt.subplot(2, 3, 2)\n",
    "        if self.test_accuracies:\n",
    "            plt.plot(self.test_epochs, self.test_accuracies, 'g-', marker='o', label='Accuracy')\n",
    "            plt.plot(self.test_epochs, self.test_precisions, 'b-', marker='o', label='Precision')\n",
    "            plt.plot(self.test_epochs, self.test_recalls, 'r-', marker='o', label='Recall')\n",
    "            plt.plot(self.test_epochs, self.test_f1s, 'y-', marker='o', label='F1')\n",
    "        plt.title('Classification Metrics')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        \n",
    "        # MCC and R2 plot\n",
    "        plt.subplot(2, 3, 3)\n",
    "        if self.test_mccs:\n",
    "            plt.plot(self.test_epochs, self.test_mccs, 'p-', marker='o', label='MCC')\n",
    "            plt.plot(self.test_epochs, self.test_r2s, 'y-', marker='o', label='R2')\n",
    "        plt.title('MCC and R2 Scores')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        \n",
    "        # ROC AUC plot\n",
    "        plt.subplot(2, 3, 4)\n",
    "        if self.test_roc_aucs:\n",
    "            plt.plot(self.test_epochs, self.test_roc_aucs, 'm-', marker='o', label='ROC AUC')\n",
    "        plt.title('ROC AUC Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for model evaluation\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'mcc': matthews_corrcoef(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Calculate ROC AUC if probabilities are provided\n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "    else:\n",
    "        metrics['roc_auc'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Plot ROC curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba[:, 1])\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=['Non-Fraud', 'Fraud']):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with percentages and counts\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotation text with both count and percentage\n",
    "    annot = np.empty_like(cm, dtype=str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            annot[i, j] = f'{cm[i, j]}\\n({cm_percent[i, j]:.1f}%)'\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix\\nCount and (Percentage)')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Model architectures remain the same as before\n",
    "class GraphSAGELayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True):\n",
    "        super(GraphSAGELayer, self).__init__()\n",
    "        self.normalize = normalize\n",
    "        self.weight = nn.Linear(in_channels * 2, out_channels, bias=True)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.weight)\n",
    "        if self.weight.bias is not None:\n",
    "            nn.init.zeros_(self.weight.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        neighbor_features = x[col]\n",
    "        \n",
    "        # Calculate mean of neighbor features\n",
    "        neighbor_mean = scatter_mean(neighbor_features, row, dim=0, dim_size=x.size(0))\n",
    "        \n",
    "        # Concatenate node's own features with neighbor features\n",
    "        out = torch.cat([x, neighbor_mean], dim=1)\n",
    "        out = self.weight(out)\n",
    "        \n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2, dim=1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def scatter_mean(src, index, dim=-1, dim_size=None):\n",
    "    if dim_size is None:\n",
    "        dim_size = index.max().item() + 1\n",
    "    \n",
    "    out = torch.zeros((dim_size, src.size(-1)), device=src.device)\n",
    "    count = torch.zeros(dim_size, device=src.device)\n",
    "    \n",
    "    index_expanded = index.unsqueeze(-1).expand(-1, src.size(-1))\n",
    "    out.scatter_add_(0, index_expanded, src)\n",
    "    count.scatter_add_(0, index, torch.ones_like(index, dtype=torch.float))\n",
    "    \n",
    "    count[count == 0] = 1\n",
    "    count = count.unsqueeze(-1)\n",
    "    \n",
    "    return out / count\n",
    "\n",
    "class GraphSAGEGenerator(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_layers=2):\n",
    "        super(GraphSAGEGenerator, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.fc_z = nn.Linear(latent_size, hidden_size)\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = hidden_size if i == 0 else hidden_size\n",
    "            self.conv_layers.append(GraphSAGELayer(in_channels, hidden_size))\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        x = self.fc_z(z)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.conv_layers[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class GraphSAGEDiscriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super(GraphSAGEDiscriminator, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        first_layer = GraphSAGELayer(input_size, hidden_size)\n",
    "        self.conv_layers.append(first_layer)\n",
    "        \n",
    "        for i in range(num_layers - 1):\n",
    "            self.conv_layers.append(GraphSAGELayer(hidden_size, hidden_size))\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.conv_layers[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Data preprocessing functions\n",
    "def df_label_encoder(df, columns):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in columns:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df_label_encoder(df, ['merchant', 'category', 'city', 'state', 'job'])\n",
    "    df['amt'] = (df['amt'] - df['amt'].min()) / (df['amt'].max() - df['amt'].min())\n",
    "    df['node_from'] = df['cc_num'].astype(str)\n",
    "    df['node_to'] = df['merchant'].astype(str)\n",
    "    df = df.sort_values(by=['node_from'])\n",
    "    node_list = pd.concat([df['node_from'], df['node_to']]).unique()\n",
    "    return df, node_list\n",
    "\n",
    "def create_graph_data(df, node_list):\n",
    "    node_map = {node: idx for idx, node in enumerate(node_list)}\n",
    "    edge_index = np.array([[node_map[from_node], node_map[to_node]] \n",
    "                          for from_node, to_node in zip(df['node_from'], df['node_to'])], \n",
    "                         dtype=np.int64).T\n",
    "    \n",
    "    node_features = torch.tensor(df[['amt', 'category', 'city', 'state']].values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    labels = torch.tensor(df['is_fraud'].values, dtype=torch.long)\n",
    "    \n",
    "    return node_features, edge_index, labels\n",
    "\n",
    "def train_wgan_graphsage(generator, discriminator, node_features, edge_index, labels, \n",
    "                        num_epochs=16, batch_size=32, critic_iterations=5):\n",
    "    device = node_features.device\n",
    "    \n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    \n",
    "    real_data = node_features[labels == 1]\n",
    "    latent_size = generator.fc_z.in_features\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train Discriminator\n",
    "        for _ in range(critic_iterations):\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            d_real = discriminator(real_data, edge_index)\n",
    "            z = torch.randn(real_data.size(0), latent_size, device=device)\n",
    "            fake_data = generator(z, edge_index)\n",
    "            d_fake = discriminator(fake_data.detach(), edge_index)\n",
    "            \n",
    "            loss_d = -torch.mean(d_real) + torch.mean(d_fake)\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Clip discriminator weights\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        z = torch.randn(real_data.size(0), latent_size, device=device)\n",
    "        fake_data = generator(z, edge_index)\n",
    "        loss_g = -torch.mean(discriminator(fake_data, edge_index))\n",
    "        loss_g.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wgan_graphsage(generator, discriminator, node_features, edge_index, labels, \n",
    "                        num_epochs=16, batch_size=32, critic_iterations=5, tracker=None):\n",
    "    device = node_features.device\n",
    "    \n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    \n",
    "    real_data = node_features[labels == 1]\n",
    "    latent_size = generator.fc_z.in_features\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop for Discriminator\n",
    "        for _ in range(critic_iterations):\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            # Real data predictions\n",
    "            d_real = discriminator(real_data, edge_index)\n",
    "            z = torch.randn(real_data.size(0), latent_size, device=device)\n",
    "            \n",
    "            # Generate fake data\n",
    "            fake_data = generator(z, edge_index)\n",
    "            d_fake = discriminator(fake_data.detach(), edge_index)\n",
    "            \n",
    "            # Discriminator loss\n",
    "            loss_d = -torch.mean(d_real) + torch.mean(d_fake)\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Clip discriminator weights\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        # Training loop for Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        z = torch.randn(real_data.size(0), latent_size, device=device)\n",
    "        fake_data = generator(z, edge_index)\n",
    "        loss_g = -torch.mean(discriminator(fake_data, edge_index))\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # Logging losses\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}\")\n",
    "        \n",
    "        if tracker:\n",
    "            # Optionally track metrics if a tracker is provided\n",
    "            tracker.update(epoch, train_loss=loss_d.item(), test_loss=loss_g.item())\n",
    "\n",
    "    # Optionally, plot the metrics after training\n",
    "    if tracker:\n",
    "        tracker.plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_metrics(generator, classifier, node_features, edge_index, labels, tracker=None):\n",
    "    device = node_features.device\n",
    "    \n",
    "    # Split data into real and generated samples\n",
    "    real_data = node_features[labels == 1]\n",
    "    latent_size = generator.fc_z.in_features\n",
    "    z = torch.randn(real_data.size(0), latent_size, device=device)\n",
    "    \n",
    "    # Generate synthetic fraud samples\n",
    "    generated_data = generator(z, edge_index).detach()\n",
    "    \n",
    "    # Combine real and generated data for evaluation\n",
    "    combined_data = torch.cat([real_data, generated_data], dim=0)\n",
    "    combined_labels = torch.cat([torch.ones(real_data.size(0), dtype=torch.long, device=device),\n",
    "                                 torch.zeros(generated_data.size(0), dtype=torch.long, device=device)])\n",
    "    \n",
    "    # Make predictions using the classifier\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(combined_data)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(combined_labels.cpu(), preds.cpu(), probs)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"MCC: {metrics['mcc']:.4f}\")\n",
    "    print(f\"R2 Score: {metrics['r2']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Optionally update tracker with final metrics\n",
    "    if tracker:\n",
    "        tracker.update(epoch='Final', train_loss=0, metrics=metrics)\n",
    "        tracker.plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 634.00 MiB (GPU 0; 31.74 GiB total capacity; 65.77 MiB already allocated; 404.88 MiB free; 82.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-976d1307862d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the WGAN-GNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_wgan_graphsage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Evaluate final metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d5d27e7ee808>\u001b[0m in \u001b[0;36mtrain_wgan_graphsage\u001b[0;34m(generator, discriminator, node_features, edge_index, labels, num_epochs, batch_size, critic_iterations, tracker)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Real data predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0md_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e55f61b16ae6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e55f61b16ae6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mneighbor_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mneighbor_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbor_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 634.00 MiB (GPU 0; 31.74 GiB total capacity; 65.77 MiB already allocated; 404.88 MiB free; 82.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Create a MetricsTracker instance\n",
    "tracker = MetricsTracker()\n",
    "\n",
    "# Train the WGAN-GNN model\n",
    "train_wgan_graphsage(generator, discriminator, node_features, edge_index, labels, tracker=tracker)\n",
    "\n",
    "# Evaluate final metrics\n",
    "evaluate_final_metrics(generator, classifier, node_features, edge_index, labels, tracker=tracker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    # Existing metrics calculation\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'mcc': matthews_corrcoef(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Calculate ROC AUC if probabilities are provided\n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "    else:\n",
    "        metrics['roc_auc'] = 0.0  # default to zero if no probabilities available\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jensen-Shannon Divergence (Mode Collapse Score): 0.1677\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "def calculate_jsd(true_data, generated_data, bins=100):\n",
    "    \"\"\"\n",
    "    Compute the Jensen-Shannon Divergence (JSD) between the true and generated data distributions.\n",
    "    \n",
    "    :param true_data: Tensor or numpy array containing the true data.\n",
    "    :param generated_data: Tensor or numpy array containing the generated data.\n",
    "    :param bins: Number of bins to discretize the data.\n",
    "    \n",
    "    :return: Jensen-Shannon Divergence value.\n",
    "    \"\"\"\n",
    "    # Flatten and convert to numpy arrays\n",
    "    true_data = true_data.flatten().cpu().detach().numpy()\n",
    "    generated_data = generated_data.flatten().cpu().detach().numpy()\n",
    "    \n",
    "    # Compute the histograms for true and generated data\n",
    "    true_hist, _ = np.histogram(true_data, bins=bins, density=True)\n",
    "    gen_hist, _ = np.histogram(generated_data, bins=bins, density=True)\n",
    "    \n",
    "    # Add a small value to avoid zero probabilities (log(0) issue)\n",
    "    epsilon = 1e-8\n",
    "    true_hist = np.clip(true_hist, epsilon, 1.0)\n",
    "    gen_hist = np.clip(gen_hist, epsilon, 1.0)\n",
    "    \n",
    "    # Normalize histograms to ensure they sum to 1\n",
    "    true_hist /= true_hist.sum()\n",
    "    gen_hist /= gen_hist.sum()\n",
    "    \n",
    "    # Compute the Jensen-Shannon Divergence\n",
    "    jsd = jensenshannon(true_hist, gen_hist)\n",
    "    \n",
    "    return jsd\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate some data for testing\n",
    "    true_data = torch.randn(1000)  # Example true data (e.g., real samples)\n",
    "    generated_data = torch.randn(1000)  # Example generated data (e.g., fake samples)\n",
    "    \n",
    "    jsd_score = calculate_jsd(true_data, generated_data, bins=100)\n",
    "    print(f\"Jensen-Shannon Divergence (Mode Collapse Score): {jsd_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_augmented' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e837c8df4dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Plot class distribution after data augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplot_class_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Class Distribution After Data Augmentation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_augmented' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to plot class distribution\n",
    "def plot_class_distribution(labels, title=\"Class Distribution\"):\n",
    "    unique_classes, counts = torch.unique(labels, return_counts=True)\n",
    "    class_names = [\"Non-Fraud\", \"Fraud\"]  # Assuming class 0 is \"Non-Fraud\" and class 1 is \"Fraud\"\n",
    "    plt.bar(class_names, counts.numpy(), color=['blue', 'orange'])\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot class distribution after data augmentation\n",
    "plot_class_distribution(y_augmented, title=\"Class Distribution After Data Augmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a9dee3d64f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Ensure real and generated data have the same number of samples for R-squared calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmin_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mreal_data_sampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mgenerated_data_sampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'real_data' is not defined"
     ]
    }
   ],
   "source": [
    "# In[28]:\n",
    "\n",
    "# Function to compute mean, variance, and standard deviation\n",
    "def compute_statistics(features):\n",
    "    mean = torch.mean(features, dim=0)\n",
    "    var = torch.var(features, dim=0)\n",
    "    std = torch.std(features, dim=0)\n",
    "    return mean, var, std\n",
    "\n",
    "# Function to compute R-squared performance metrics\n",
    "def r_squared(real_data, generated_data):\n",
    "    ss_res = torch.sum((real_data - generated_data) ** 2, dim=0)  # Residual sum of squares\n",
    "    ss_tot = torch.sum((real_data - torch.mean(real_data, dim=0)) ** 2, dim=0)  # Total sum of squares\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "# Ensure real and generated data have the same number of samples for R-squared calculation\n",
    "min_size = min(real_data.size(0), generated_data.size(0))\n",
    "real_data_sampled = real_data[:min_size]\n",
    "generated_data_sampled = generated_data[:min_size]\n",
    "\n",
    "# Compute statistics and R-squared metrics\n",
    "final_mean, final_var, final_std = compute_statistics(x_augmented)\n",
    "r2_scores = r_squared(real_data_sampled, generated_data_sampled)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"Mean (per feature): {final_mean}\")\n",
    "print(f\"Variance (per feature): {final_var}\")\n",
    "print(f\"Standard Deviation (per feature): {final_std}\")\n",
    "print(f\"R-squared (per feature): {r2_scores}\")\n",
    "print(f\"Mean R-squared: {r2_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
